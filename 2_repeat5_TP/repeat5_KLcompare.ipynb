{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_ALL_ldKL = np.loadtxt(\"BTP_ALL_ldKL.csv\", delimiter = \",\", dtype = float)\n",
    "TP_ALL_liKL = np.loadtxt(\"BTP_ALL_liKL.csv\", delimiter = \",\", dtype = float)\n",
    "TP_ALL_ncKL = np.loadtxt(\"BTP_ALL_ncKL.csv\", delimiter = \",\", dtype = float)\n",
    "TP_ONE_cKL = np.loadtxt(\"BTP_ONE_cKL.csv\", delimiter = \",\", dtype = float)\n",
    "TP_ONE_ldKL = np.loadtxt(\"BTP_ONE_ldKL.csv\", delimiter = \",\", dtype = float)\n",
    "TP_ONE_qdKL = np.loadtxt(\"BTP_ONE_qdKL.csv\", delimiter = \",\", dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_ALL_ldKL_a = np.sum(TP_ALL_ldKL, axis = -1)\n",
    "TP_ALL_liKL_a = np.sum(TP_ALL_liKL, axis = -1)\n",
    "TP_ALL_ncKL_a = np.sum(TP_ALL_ncKL, axis = -1)\n",
    "TP_ONE_cKL_a = np.sum(TP_ONE_cKL, axis = -1)\n",
    "TP_ONE_ldKL_a = np.sum(TP_ONE_ldKL, axis = -1)\n",
    "TP_ONE_qdKL_a = np.sum(TP_ONE_qdKL, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.array([\"1\", \"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\", \"100\"])\n",
    "\n",
    "fig = plt.figure(figsize=(25, 8))\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "ax.set_title(\"Constant KL divergence, 0.1 mean shift for only X1\")\n",
    "ax.plot(x_axis, TP_ONE_cKL_a)\n",
    "ax.set_xlabel(\"dimension\")\n",
    "ax.set_ylabel(\"power\")\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.set_title(\"increase KL divergence, mean shift for all Xi, meanXi = 1 / i\")\n",
    "ax1.plot(x_axis, TP_ALL_ncKL_a)\n",
    "ax1.set_xlabel(\"dimension\")\n",
    "ax1.set_ylabel(\"power\")\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.set_title(\"Linear increase KL divergence, 0.01 mean shift for all Xi\")\n",
    "ax2.plot(x_axis, TP_ALL_liKL_a)\n",
    "ax2.set_xlabel(\"dimension\")\n",
    "ax2.set_ylabel(\"power\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(25, 8))\n",
    "\n",
    "ax3 = fig.add_subplot(131)\n",
    "ax3.set_title(\"Linear decrease KL divergence, mean shift for all Xi, meanXi = 1 / dim\")\n",
    "ax3.plot(x_axis, TP_ALL_ldKL_a)\n",
    "ax3.set_xlabel(\"dimension\")\n",
    "ax3.set_ylabel(\"power\")\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(132)\n",
    "ax4.set_title(\"Linear decrease KL divergence, mean shift for only X1, meanX1 = 1 / dim^0.5\")\n",
    "ax4.plot(x_axis, TP_ONE_ldKL_a)\n",
    "ax4.set_xlabel(\"dimension\")\n",
    "ax4.set_ylabel(\"power\")\n",
    " \n",
    "\n",
    "ax5 = fig.add_subplot(133)\n",
    "ax5.set_title(\"Quadratic decrease KL divergence, mean shift for only X1, meanX1 = 1 / dim\")\n",
    "ax5.plot(x_axis, TP_ONE_qdKL_a)\n",
    "ax5.set_xlabel(\"dimension\")\n",
    "ax5.set_ylabel(\"power\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
